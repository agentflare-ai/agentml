---
title: "Ollama Extension"
description: "Use local LLM models with AgentML via Ollama"
---

# Ollama Extension

The Ollama namespace (`github.com/agentflare-ai/agentml-go/ollama`) enables AgentML agents to use locally-hosted Ollama models, providing privacy and cost-effective LLM capabilities.

## Prerequisites

Install and run Ollama:

```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull a model
ollama pull llama2

# Start Ollama server (runs on http://localhost:11434 by default)
ollama serve
```

## Basic Usage

```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:ollama="github.com/agentflare-ai/agentml-go/ollama">

  <datamodel>
    <data id="userInput" expr="''" />
    <data id="response" expr="''" />
  </datamodel>

  <state id="process">
    <onentry>
      <ollama:generate
        model="llama2"
        location="_event"
        promptexpr="'You are a helpful assistant. User: ' + userInput" />
    </onentry>

    <transition event="action.response" target="complete">
      <assign location="response" expr="_event.data.message" />
    </transition>

    <transition event="action.error" target="error_handler" />
  </state>
</agent>
```

## Actions

### `<ollama:generate>`

Generate text with local Ollama models:

**Attributes:**
- `model` (required) - Model name (e.g., "llama2", "mistral", "codellama")
- `location` (required) - Variable to store event reference
- `promptexpr` (required) - ECMAScript expression for the prompt

**Events raised:**
- `action.response` - Generation completed
  - `_event.data.message` - Generated text
- `action.error` - Generation failed
  - `_event.data.error` - Error details

## Available Models

### Llama 2

Meta's open-source model:

```bash
# Pull different sizes
ollama pull llama2        # 7B parameters
ollama pull llama2:13b    # 13B parameters
ollama pull llama2:70b    # 70B parameters
```

```xml
<ollama:generate
  model="llama2"
  location="_event"
  promptexpr="userQuery" />
```

### Mistral

High-performance open model:

```bash
ollama pull mistral
```

```xml
<ollama:generate
  model="mistral"
  location="_event"
  promptexpr="userQuery" />
```

### Code Llama

Specialized for code generation:

```bash
ollama pull codellama
```

```xml
<ollama:generate
  model="codellama"
  location="_event"
  promptexpr="'Write a Python function that: ' + task" />
```

### Custom Models

Use your own fine-tuned models:

```bash
# Create Modelfile
cat > Modelfile <<EOF
FROM llama2
PARAMETER temperature 0.8
SYSTEM "You are a specialized assistant for..."
EOF

# Build custom model
ollama create my-custom-model -f Modelfile

# Pull from registry
ollama pull username/model-name
```

```xml
<ollama:generate
  model="my-custom-model"
  location="_event"
  promptexpr="userQuery" />
```

## Hybrid: Local + Cloud

Use Ollama for classification, Gemini for generation:

```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:ollama="github.com/agentflare-ai/agentml-go/ollama"
       xmlns:gemini="github.com/agentflare-ai/agentml/gemini">

  <state id="classify">
    <!-- Fast local classification -->
    <onentry>
      <ollama:generate
        model="llama2"
        location="_event"
        promptexpr="'Classify this as simple or complex: ' + userInput" />
    </onentry>

    <transition event="action.response"
                cond="_event.data.message.includes('complex')"
                target="cloud_generation" />

    <transition event="action.response" target="local_generation" />
  </state>

  <state id="cloud_generation">
    <!-- Use Gemini for complex queries -->
    <onentry>
      <gemini:generate
        model="gemini-2.0-flash-exp"
        location="_event"
        promptexpr="'Provide detailed analysis: ' + userInput" />
    </onentry>

    <transition event="action.response" target="respond" />
  </state>

  <state id="local_generation">
    <!-- Use Ollama for simple queries -->
    <onentry>
      <ollama:generate
        model="llama2"
        location="_event"
        promptexpr="'Provide brief answer: ' + userInput" />
    </onentry>

    <transition event="action.response" target="respond" />
  </state>
</agent>
```

## Error Handling

Handle Ollama-specific errors:

```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       xmlns:ollama="github.com/agentflare-ai/agentml-go/ollama">

  <state id="generate">
    <onentry>
      <ollama:generate
        model="llama2"
        location="_event"
        promptexpr="userQuery" />
    </onentry>

    <!-- Success -->
    <transition event="action.response" target="success" />

    <!-- Model not found -->
    <transition event="action.error"
                cond="_event.data.error.type === 'model_not_found'"
                target="model_missing">
      <log expr="'Model llama2 not found. Please run: ollama pull llama2'" />
    </transition>

    <!-- Connection failed -->
    <transition event="action.error"
                cond="_event.data.error.type === 'connection_failed'"
                target="ollama_offline">
      <log expr="'Cannot connect to Ollama. Is it running?'" />
    </transition>

    <!-- Other errors -->
    <transition event="action.error" target="failed" />
  </state>
</agent>
```

## Configuration

Configure Ollama server URL:

```bash
# Default (localhost)
agentmlx run agent.aml

# Custom Ollama server
export OLLAMA_BASE_URL=http://ollama-server:11434
agentmlx run agent.aml
```

## Best Practices

1. **Choose appropriate models**: Balance size and performance
2. **Pull models first**: Ensure models are downloaded before running
3. **Handle offline scenarios**: Add error handling for connection failures
4. **Monitor performance**: Local models vary in speed by hardware
5. **Use hybrid approach**: Combine local and cloud models strategically
6. **Test model sizes**: Try different parameter counts (7B, 13B, 70B)
7. **Update regularly**: Keep Ollama and models up to date
8. **Optimize hardware**: Use GPU acceleration when available

## Model Management

```bash
# List installed models
ollama list

# Pull a model
ollama pull llama2

# Remove a model
ollama rm old-model

# Show model information
ollama show llama2

# Update a model
ollama pull llama2
```

## Next Steps

- Learn about [Gemini Extension](/extensions/gemini) for cloud models
- Explore [Memory Extension](/extensions/memory) for context augmentation
- Read [Extensions Overview](/extensions/overview) for combining extensions
- See [Performance](/best-practices/performance) for optimization tips
