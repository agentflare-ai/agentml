---
title: "Extensions Overview"
description: "Extending AgentML with namespace-based extensions for LLM, memory, and I/O operations"
---

# Extensions Overview

AgentML extensions are namespaces that add custom actions to your agents. They integrate seamlessly through the `xmlns:prefix="uri"` directive, providing modular capabilities without modifying core SCXML functionality.

## What Are Extensions?

Extensions in AgentML are namespace implementations that provide XML actions for your agents:

```xml
<agent xmlns="github.com/agentflare-ai/agentml/agent"
       datamodel="ecmascript"
       xmlns:gemini="github.com/agentflare-ai/agentml/gemini"
       xmlns:memory="github.com/agentflare-ai/agentml/memory">

  <state id="process">
    <onentry>
      <!-- Use Gemini extension -->
      <gemini:generate
        model="gemini-2.0-flash-exp"
        location="_event"
        promptexpr="userInput" />

      <!-- Use Memory extension -->
      <memory:put key="last_input" expr="userInput" />
    </onentry>
  </state>
</agent>
```

## Official Extensions

### Gemini Extension

Google Gemini LLM integration:

```xml
<agent xmlns="github.com/agentflare-ai/agentml/agent"
       xmlns:gemini="github.com/agentflare-ai/agentml/gemini">

  <state id="generate">
    <onentry>
      <gemini:generate
        model="gemini-2.0-flash-exp"
        location="_event"
        promptexpr="'Explain quantum computing'" />
    </onentry>

    <transition event="action.response" target="process" />
  </state>
</agent>
```

**Environment setup:**
```bash
export GEMINI_API_KEY=your_key_here
agentmlx run agent.aml
```

[Learn more about Gemini Extension â†’](/extensions/gemini)

### Ollama Extension

Local LLM integration via Ollama:

```xml
<agent xmlns="github.com/agentflare-ai/agentml/agent"
       xmlns:ollama="github.com/agentflare-ai/agentml/ollama">

  <state id="classify">
    <onentry>
      <ollama:generate
        model="llama2"
        location="_event"
        promptexpr="'Classify: ' + userInput" />
    </onentry>

    <transition event="action.response" target="handle_classification" />
  </state>
</agent>
```

**Prerequisites:**
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull model
ollama pull llama2

# Run agent
agentmlx run agent.aml
```

[Learn more about Ollama Extension â†’](/extensions/ollama)

### Memory Extension

Vector search and graph database powered by sqlite-graph:

```xml
<agent xmlns="github.com/agentflare-ai/agentml/agent"
       xmlns:memory="github.com/agentflare-ai/agentml/memory">

  <state id="store_and_search">
    <onentry>
      <!-- Generate embedding -->
      <memory:embed location="embedding" expr="textContent" />

      <!-- Vector search -->
      <memory:search location="results" expr="queryEmbedding" limit="5" />

      <!-- Graph query -->
      <memory:graph-query location="graph_results">
        <query>
          MATCH (p:Person)-[:KNOWS]->(friend)
          RETURN p.name, friend.name
        </query>
      </memory:graph-query>
    </onentry>
  </state>
</agent>
```

[Learn more about Memory Extension â†’](/extensions/memory)

### Stdin Extension

Console I/O for terminal agents:

```xml
<agent xmlns="github.com/agentflare-ai/agentml/agent"
       xmlns:stdin="github.com/agentflare-ai/agentml/stdin">

  <state id="interactive">
    <onentry>
      <stdin:write expr="'Bot: How can I help?'" />
      <stdin:read location="userInput" />
    </onentry>

    <transition event="stdin.received" target="process" />
  </state>
</agent>
```

## Extension Architecture

Extensions are namespaces that:
1. Register custom XML actions
2. Execute during state machine transitions
3. Raise events back to the agent
4. Manage external resources (LLM APIs, databases, I/O)

### Event Flow

```xml
<state id="example">
  <onentry>
    <!-- 1. Action executes -->
    <gemini:generate model="gemini-2.0-flash-exp" location="_event" promptexpr="'Hello'" />
  </onentry>

  <!-- 2. Extension raises event -->
  <transition event="action.response" target="success">
    <!-- 3. Access event data -->
    <assign location="result" expr="_event.data.message" />
  </transition>

  <!-- 4. Handle errors -->
  <transition event="action.error" target="error_handler" />
</state>
```

## Multiple Extensions

Combine extensions for powerful functionality:

```xml
<agent xmlns="github.com/agentflare-ai/agentml/agent"
       xmlns:gemini="github.com/agentflare-ai/agentml/gemini"
       xmlns:ollama="github.com/agentflare-ai/agentml/ollama"
       xmlns:memory="github.com/agentflare-ai/agentml/memory"
       xmlns:stdin="github.com/agentflare-ai/agentml/stdin">

  <state id="interactive">
    <onentry>
      <stdin:read location="userInput" />
    </onentry>

    <transition event="stdin.received" target="classify" />
  </state>

  <state id="classify">
    <onentry>
      <!-- Fast local classification -->
      <ollama:generate model="llama2" location="_event" promptexpr="'Classify: ' + userInput" />
    </onentry>

    <transition event="action.response"
                cond="_event.data.category === 'complex'"
                target="deep_processing" />

    <transition event="action.response" target="simple_processing" />
  </state>

  <state id="deep_processing">
    <onentry>
      <!-- Search memory for context -->
      <memory:embed location="embedding" expr="userInput" />
      <memory:search location="context" expr="embedding" limit="3" />

      <!-- Generate with Gemini and context -->
      <gemini:generate
        model="gemini-2.0-flash-exp"
        location="_event"
        promptexpr="'Context: ' + JSON.stringify(context) + '\nQuestion: ' + userInput" />
    </onentry>

    <transition event="action.response" target="respond" />
  </state>

  <state id="respond">
    <onentry>
      <stdin:write expr="'Bot: ' + response" />
      <memory:put key="last_response" expr="response" />
    </onentry>

    <transition target="interactive" />
  </state>
</agent>
```

## Creating Custom Extensions

Build custom namespaces in Go:

**myextension/actions.go:**
```go
package myextension

import (
    "github.com/agentflare-ai/agentmlx/pkg/scxml"
)

type ProcessAction struct {
    Model    string `xml:"model,attr"`
    Location string `xml:"location,attr"`
    Input    string `xml:"inputexpr,attr"`
}

func (a *ProcessAction) Execute(ctx *scxml.Context) error {
    // Get input from datamodel
    input := ctx.EvalExpression(a.Input)

    // Process with custom logic
    result := customProcess(a.Model, input)

    // Store result
    ctx.SetVariable(a.Location, result)

    // Raise completion event
    ctx.RaiseEvent("custom.complete", map[string]interface{}{
        "result": result,
    })

    return nil
}

func Register() {
    scxml.RegisterNamespace("github.com/example/myextension", map[string]interface{}{
        "process": &ProcessAction{},
    })
}
```

**Usage:**
```xml
<agent xmlns="github.com/agentflare-ai/agentml/agent"
       xmlns:custom="github.com/example/myextension">

  <state id="use_custom">
    <onentry>
      <custom:process
        model="mymodel"
        location="_event"
        inputexpr="userInput" />
    </onentry>

    <transition event="custom.complete" target="process_result" />
  </state>
</agent>
```

[Learn more about Custom Extensions â†’](/extensions/custom)

## Extension Configuration

Configure extensions via environment variables or command-line:

```bash
# Gemini API key
export GEMINI_API_KEY=your_key

# Ollama server URL
export OLLAMA_BASE_URL=http://localhost:11434

# Memory database path
export AGENTML_MEMORY_DB=/path/to/memory.db

# Run agent
agentmlx run agent.aml
```

## Available Extensions

| Extension | Purpose | Status |
|-----------|---------|--------|
| **gemini** | Google Gemini LLM integration | âœ… Stable |
| **ollama** | Local LLM via Ollama | âœ… Stable |
| **memory** | Vector search + graph database | âœ… Stable |
| **stdin** | Console I/O | âœ… Stable |
| **http** | HTTP I/O Processor | âœ… Stable |
| **websocket** | WebSocket I/O Processor | ðŸš§ Beta |

## Best Practices

1. **Import only what you need**: Don't import unused namespaces
2. **Handle errors**: Add error transitions for all extension actions
3. **Use schemas**: Define `event:schema` for extension-generated events
4. **Monitor resources**: Track API usage, memory consumption
5. **Test thoroughly**: Validate extension behavior in all states
6. **Document dependencies**: Note required environment variables
7. **Version carefully**: Pin extension versions for stability
8. **Secure credentials**: Never hardcode API keys in agent files

## Extension Registry (Planned)

Future: Centralized registry for discovering extensions:

```bash
# Search for extensions
agentmlx extension search stripe

# Install extension
agentmlx extension install github.com/community/stripe

# List installed
agentmlx extension list

# Update extension
agentmlx extension update github.com/community/stripe
```

## Next Steps

- Explore [Gemini Extension](/extensions/gemini) for cloud LLM integration
- Learn about [Ollama Extension](/extensions/ollama) for local models
- Discover [Memory Extension](/extensions/memory) for vector search
- Build [Custom Extensions](/extensions/custom) for specialized needs
- Read [Namespace System](/architecture/namespace-system) for technical details
