---
title: "Docker Deployment"
description: "Containerize and deploy AgentML agents with Docker"
---

# Docker Deployment

Package AgentML agents as Docker containers for portable, consistent deployments across any infrastructure.

## Basic Dockerfile

**Dockerfile:**
```dockerfile
FROM debian:bookworm-slim

# Install dependencies
RUN apt-get update && apt-get install -y \
    ca-certificates \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install agentmlx
RUN curl -fsSL https://raw.githubusercontent.com/agentflare-ai/agentml/main/install.sh | sh

# Create app directory
WORKDIR /app

# Copy agent files
COPY agents/ /app/agents/

# Set environment
ENV PATH="/root/.agentml/bin:${PATH}"

# Expose port if agent uses HTTP I/O
EXPOSE 8080

# Run agent
CMD ["agentmlx", "run", "agents/main.aml"]
```

## Build and Run

```bash
# Build image
docker build -t my-agent:latest .

# Run container
docker run -d \
  --name my-agent \
  -e GEMINI_API_KEY=your_key \
  -p 8080:8080 \
  my-agent:latest

# View logs
docker logs -f my-agent
```

## Docker Compose

For multi-agent systems:

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  orchestrator:
    build: .
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - WORKER_URL=http://worker:8080
    ports:
      - "8080:8080"
    depends_on:
      - worker
      - memory
    command: ["agentmlx", "run", "agents/orchestrator.aml"]

  worker:
    build: .
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - MEMORY_DB=/data/memory.db
    volumes:
      - worker_data:/data
    command: ["agentmlx", "run", "agents/worker.aml"]
    deploy:
      replicas: 3

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama

  memory:
    image: postgres:16-alpine
    environment:
      - POSTGRES_DB=agentml
      - POSTGRES_PASSWORD=secure_password
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  worker_data:
  ollama_models:
  postgres_data:
```

Start all services:

```bash
docker-compose up -d
```

## Multi-Stage Build

Optimize image size:

**Dockerfile:**
```dockerfile
# Stage 1: Download agentmlx
FROM debian:bookworm-slim AS installer

RUN apt-get update && apt-get install -y curl ca-certificates
RUN curl -fsSL https://raw.githubusercontent.com/agentflare-ai/agentml/main/install.sh | sh

# Stage 2: Runtime
FROM debian:bookworm-slim

# Copy only agentmlx binary
COPY --from=installer /root/.agentml/bin/agentmlx /usr/local/bin/

# Copy agent files
WORKDIR /app
COPY agents/ /app/agents/

# Run as non-root
RUN useradd -m -u 1000 agent && chown -R agent:agent /app
USER agent

CMD ["agentmlx", "run", "agents/main.aml"]
```

## Health Checks

Add health checks for container orchestration:

```dockerfile
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD agentmlx validate agents/main.aml || exit 1
```

Or with HTTP endpoint:

```dockerfile
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8080/health || exit 1
```

## Kubernetes Deployment

**deployment.yaml:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agentml-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: agentml
  template:
    metadata:
      labels:
        app: agentml
    spec:
      containers:
      - name: agent
        image: my-agent:latest
        ports:
        - containerPort: 8080
        env:
        - name: GEMINI_API_KEY
          valueFrom:
            secretKeyRef:
              name: agentml-secrets
              key: gemini-api-key
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: agentml-service
spec:
  selector:
    app: agentml
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
```

Deploy:

```bash
kubectl apply -f deployment.yaml
```

## Secrets Management

Store API keys securely:

```bash
# Create secret
kubectl create secret generic agentml-secrets \
  --from-literal=gemini-api-key=your_key_here

# Or from file
kubectl create secret generic agentml-secrets \
  --from-env-file=.env
```

## Best Practices

1. **Use multi-stage builds**: Minimize image size
2. **Run as non-root**: Improve security
3. **Set resource limits**: Prevent resource exhaustion
4. **Use .dockerignore**: Exclude unnecessary files
5. **Tag images properly**: Use semantic versioning
6. **Health checks**: Enable container orchestration features
7. **Log to stdout**: Enable log aggregation
8. **Mount volumes**: Persist memory database if needed

**.dockerignore:**
```
.git
node_modules
*.log
.env
README.md
docs/
```

## Monitoring

Use Prometheus for metrics:

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'agentml-agents'
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['default']
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        regex: agentml
        action: keep
```

## Next Steps

- Try [Vercel Deployment](/deployment/vercel) for serverless
- Learn about [Self-Hosted](/deployment/self-hosted) deployment
- Read [Performance](/best-practices/performance) for optimization
