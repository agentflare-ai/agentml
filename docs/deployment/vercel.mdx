---
title: "Deploy to Vercel"
description: "Deploy AgentML agents as serverless functions on Vercel"
---

# Deploy to Vercel

Deploy AgentML agents as Vercel serverless functions for auto-scaling, edge deployment, and zero configuration.

## Overview

AgentML agents can run as Vercel serverless functions by wrapping the agentmlx runtime in a function handler. This provides:
- Auto-scaling based on demand
- Global edge deployment
- Zero infrastructure management
- Pay-per-execution pricing

## Project Structure

```
project/
├── agents/
│   └── chatbot.aml          # Your AgentML agent
├── api/
│   └── agent.ts             # Vercel function handler
├── package.json
└── vercel.json
```

## Vercel Function Handler

**api/agent.ts:**
```typescript
import { exec } from 'child_process'
import { promisify } from 'util'
import { writeFile } from 'fs/promises'

const execAsync = promisify(exec)

export const config = {
  runtime: 'nodejs18',
  maxDuration: 60, // seconds
}

export default async function handler(req, res) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' })
  }

  const { event, data } = req.body

  try {
    // Execute agentmlx with event
    const eventData = JSON.stringify({ name: event, data })
    const { stdout, stderr } = await execAsync(
      `agentmlx run agents/chatbot.aml --event '${eventData}'`,
      {
        env: {
          ...process.env,
          GEMINI_API_KEY: process.env.GEMINI_API_KEY,
        }
      }
    )

    // Parse and return result
    const result = JSON.parse(stdout)
    res.status(200).json(result)
  } catch (error) {
    console.error('Agent execution error:', error)
    res.status(500).json({ error: error.message })
  }
}
```

## Environment Variables

Set environment variables in Vercel dashboard or CLI:

```bash
# Vercel CLI
vercel env add GEMINI_API_KEY production
vercel env add OLLAMA_BASE_URL production
vercel env add AGENTML_MEMORY_DB production

# Or via dashboard at vercel.com/dashboard
```

## Configuration

**vercel.json:**
```json
{
  "functions": {
    "api/agent.ts": {
      "maxDuration": 60,
      "memory": 1024
    }
  },
  "env": {
    "GEMINI_API_KEY": "@gemini-api-key",
    "NODE_ENV": "production"
  }
}
```

## Edge Functions (Experimental)

For ultra-low latency, use Vercel Edge Functions with WASM:

**api/edge-agent.ts:**
```typescript
export const config = {
  runtime: 'edge',
}

export default async function handler(req: Request) {
  // Future: Load AgentML WASM runtime
  // const { runAgent } = await import('@agentml/wasm')

  const { event, data } = await req.json()

  // Execute agent in edge runtime
  // const result = await runAgent('agents/chatbot.aml', event, data)

  return new Response(JSON.stringify({ result }), {
    headers: { 'Content-Type': 'application/json' },
  })
}
```

## Deployment

### Automatic (Git Integration)

1. Connect repository to Vercel
2. Configure build settings
3. Every push to `main` deploys automatically

### Manual

```bash
# Install Vercel CLI
npm i -g vercel

# Deploy
vercel

# Deploy to production
vercel --prod
```

## Best Practices

1. **Keep agents small**: Faster cold starts
2. **Use environment variables**: Never commit API keys
3. **Set appropriate timeouts**: Match function duration to agent complexity
4. **Monitor usage**: Track invocations and costs
5. **Cache when possible**: Use Vercel's caching for static responses
6. **Test locally**: Use `vercel dev` for local testing

## Limitations

- **Cold starts**: First invocation may be slower
- **Execution time**: Max 60 seconds (Hobby), 300 seconds (Pro)
- **Memory**: Max 3GB
- **Stateless**: No persistent file system between invocations

## Monitoring

Use Vercel Analytics and Logs:

```bash
# View logs
vercel logs

# Monitor in real-time
vercel logs --follow
```

## Next Steps

- Try [Docker Deployment](/deployment/docker) for containerized deployment
- Learn about [Self-Hosted](/deployment/self-hosted) for full control
- Read [Performance Best Practices](/best-practices/performance) for optimization
