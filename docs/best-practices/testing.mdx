---
title: "Testing"
description: "Testing strategies for AgentML agents"
---

# Testing

Comprehensive testing ensures your AgentML agents work reliably. AgentML's SCXML foundation enables systematic testing of state machines, events, and transitions.

## Validation Testing

Validate agent syntax before deployment:

```bash
# Validate agent file
agentmlx validate agent.aml

# Validate and show details
agentmlx validate agent.aml --verbose

# Validate all agents
find agents/ -name "*.aml" -exec agentmlx validate {} \;
```

Expected output:
```
✓ Agent is valid SCXML
✓ All namespaces are registered
✓ Datamodel expressions are valid
✓ Event schemas are valid JSON Schema
```

## Unit Testing States

Test individual states with targeted events:

**test-agent.aml:**
```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript">

  <datamodel>
    <data id="result" expr="''" />
  </datamodel>

  <state id="parse_input">
    <onentry>
      <assign location="result" expr="userInput.trim().toLowerCase()" />
      <raise event="parse.complete" />
    </onentry>

    <transition event="parse.complete" target="complete" />
  </state>

  <state id="complete">
    <onentry>
      <log expr="'Result: ' + result" />
    </onentry>
  </state>
</agent>
```

**test.sh:**
```bash
#!/bin/bash

# Test valid input
result=$(agentmlx run test-agent.aml --event '{"name": "start", "data": {"userInput": "  HELLO  "}}')
echo "$result" | grep -q "hello" && echo "✓ Parse test passed" || echo "✗ Parse test failed"

# Test empty input
result=$(agentmlx run test-agent.aml --event '{"name": "start", "data": {"userInput": ""}}')
echo "$result" | grep -q '""' && echo "✓ Empty input test passed" || echo "✗ Empty input test failed"
```

## Integration Testing

Test multi-state workflows:

**integration-test.aml:**
```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:memory="github.com/agentflare-ai/agentml-go/memory">

  <datamodel>
    <data id="testPassed" expr="false" />
  </datamodel>

  <state id="store_data">
    <onentry>
      <memory:put key="test_key" expr="'test_value'" />
      <raise event="stored" />
    </onentry>

    <transition event="stored" target="retrieve_data" />
  </state>

  <state id="retrieve_data">
    <onentry>
      <memory:get key="test_key" location="retrieved" />
    </onentry>

    <transition target="verify" />
  </state>

  <state id="verify">
    <onentry>
      <assign location="testPassed" expr="retrieved === 'test_value'" />
      <log expr="testPassed ? '✓ Integration test passed' : '✗ Integration test failed'" />
    </onentry>

    <transition target="final" />
  </state>
</agent>
```

Run integration tests:
```bash
agentmlx run integration-test.aml
```

## Mocking LLM Calls

Create test agents without LLM dependencies:

**mock-agent.aml:**
```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript">

  <datamodel>
    <!-- Mock LLM responses for testing -->
    <data id="mockResponses" expr="{
      'hello': 'Hi there! How can I help?',
      'weather': 'The weather is sunny today.',
      'default': 'I can help with that.'
    }" />
  </datamodel>

  <state id="process_input">
    <onentry>
      <!-- Simulate LLM response with mock data -->
      <assign location="response" expr="mockResponses[userInput] || mockResponses['default']" />
      <raise event="action.response" />
    </onentry>

    <transition event="action.response" target="respond">
      <log expr="'Mock response: ' + response" />
    </transition>
  </state>

  <state id="respond">
    <onentry>
      <log expr="response" />
    </onentry>
    <transition target="final" />
  </state>
</agent>
```

## State Machine Testing

Test state transitions systematically:

**state-test.aml:**
```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript">

  <datamodel>
    <data id="stateLog" expr="[]" />
  </datamodel>

  <state id="idle">
    <onentry>
      <assign location="stateLog" expr="stateLog.concat(['idle'])" />
    </onentry>

    <transition event="start" target="processing" />
  </state>

  <state id="processing">
    <onentry>
      <assign location="stateLog" expr="stateLog.concat(['processing'])" />
      <raise event="done" />
    </onentry>

    <transition event="done" target="complete" />
  </state>

  <state id="complete">
    <onentry>
      <assign location="stateLog" expr="stateLog.concat(['complete'])" />
      <log expr="'State transitions: ' + stateLog.join(' -> ')" />

      <!-- Verify expected state sequence -->
      <assign location="testPassed" expr="
        stateLog.length === 3 &&
        stateLog[0] === 'idle' &&
        stateLog[1] === 'processing' &&
        stateLog[2] === 'complete'
      " />

      <log expr="testPassed ? '✓ State transition test passed' : '✗ State transition test failed'" />
    </onentry>

    <transition target="final" />
  </state>
</agent>
```

## Event Testing

Test event handling and data:

**event-test.aml:**
```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript">

  <state id="await_event">
    <transition event="user.message" target="validate_event">
      <assign location="eventData" expr="_event.data" />
    </transition>
  </state>

  <state id="validate_event">
    <onentry>
      <!-- Test event structure -->
      <assign location="hasMessage" expr="'message' in eventData" />
      <assign location="hasTimestamp" expr="'timestamp' in eventData" />

      <assign location="testPassed" expr="hasMessage && hasTimestamp" />

      <log expr="testPassed ?
        '✓ Event structure test passed' :
        '✗ Event structure test failed: missing ' +
        (!hasMessage ? 'message' : 'timestamp')" />
    </onentry>

    <transition target="final" />
  </state>
</agent>
```

Test with events:
```bash
agentmlx run event-test.aml --event '{
  "name": "user.message",
  "data": {
    "message": "Hello",
    "timestamp": 1234567890
  }
}'
```

## Schema Validation Testing

Test JSON Schema validation:

**schema-test.aml:**
```xml
<agent xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript">

  <datamodel>
    <data id="validCount" expr="0" />
    <data id="invalidCount" expr="0" />
  </datamodel>

  <state id="await_response">
    <!-- Valid schema match -->
    <transition
      event="action.response"
      event:schema='{
        "type": "object",
        "properties": {
          "name": {"type": "string"},
          "age": {"type": "number"}
        },
        "required": ["name", "age"]
      }'
      target="valid_response">
      <assign location="validCount" expr="validCount + 1" />
    </transition>

    <!-- Invalid schema -->
    <transition event="action.response" target="invalid_response">
      <assign location="invalidCount" expr="invalidCount + 1" />
    </transition>
  </state>

  <state id="valid_response">
    <onentry>
      <log expr="'✓ Schema validation passed: ' + validCount" />
    </onentry>
    <transition target="final" />
  </state>

  <state id="invalid_response">
    <onentry>
      <log expr="'✗ Schema validation failed: ' + invalidCount" />
    </onentry>
    <transition target="final" />
  </state>
</agent>
```

## Performance Testing

Measure agent execution time:

```bash
#!/bin/bash

echo "Performance test: 10 iterations"

total_time=0
iterations=10

for i in $(seq 1 $iterations); do
  start=$(date +%s%N)
  agentmlx run agent.aml --event '{"name": "start"}' > /dev/null
  end=$(date +%s%N)

  duration=$((($end - $start) / 1000000)) # Convert to ms
  total_time=$(($total_time + $duration))

  echo "Iteration $i: ${duration}ms"
done

avg_time=$(($total_time / $iterations))
echo "Average execution time: ${avg_time}ms"

# Assert performance threshold
if [ $avg_time -lt 1000 ]; then
  echo "✓ Performance test passed"
else
  echo "✗ Performance test failed: ${avg_time}ms exceeds 1000ms threshold"
fi
```

## SCXML Conformance Testing

AgentML passes 185/193 W3C SCXML tests (95.9%). Verify conformance:

```bash
# Run SCXML conformance tests
agentmlx test --conformance

# Test specific features
agentmlx test --conformance --feature datamodel
agentmlx test --conformance --feature transitions
```

## Continuous Integration

Set up CI/CD testing:

**.github/workflows/test.yml:**
```yaml
name: Test AgentML Agents

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install agentmlx
        run: curl -fsSL sh.agentml.dev | sh

      - name: Add to PATH
        run: echo "$HOME/.agentml/bin" >> $GITHUB_PATH

      - name: Validate agents
        run: |
          for agent in agents/*.aml; do
            echo "Validating $agent"
            agentmlx validate "$agent"
          done

      - name: Run tests
        run: |
          for test in tests/*.aml; do
            echo "Running $test"
            agentmlx run "$test"
          done

      - name: Performance tests
        run: bash tests/performance.sh
```

## Test Organization

Structure your tests:

```
project/
├── agents/
│   ├── chatbot.aml
│   └── workflow.aml
├── tests/
│   ├── unit/
│   │   ├── parse-test.aml
│   │   ├── validate-test.aml
│   │   └── transform-test.aml
│   ├── integration/
│   │   ├── memory-test.aml
│   │   ├── llm-test.aml
│   │   └── multi-state-test.aml
│   ├── e2e/
│   │   └── full-workflow-test.aml
│   └── performance/
│       └── benchmark.sh
└── .github/
    └── workflows/
        └── test.yml
```

## Test Coverage Goals

Target coverage for production agents:
- **States**: 100% - All states should be reachable
- **Transitions**: 90%+ - All critical paths covered
- **Events**: 80%+ - All expected events tested
- **Error paths**: 100% - All error handlers validated

## Best Practices

1. **Validate early**: Use `agentmlx validate` before deploying
2. **Test states individually**: Isolate state logic for unit tests
3. **Mock LLM calls**: Use mock data to avoid API costs in tests
4. **Test event schemas**: Verify JSON Schema validation works
5. **Measure performance**: Track execution time and token usage
6. **Use CI/CD**: Automate testing on every commit
7. **Test error paths**: Ensure error handling works correctly
8. **Version test data**: Track test inputs and expected outputs
9. **Document tests**: Explain what each test validates
10. **Run conformance tests**: Verify SCXML compliance

## Next Steps

- Learn about [Error Handling](/best-practices/error-handling) strategies
- Read [Performance](/best-practices/performance) optimization techniques
- Explore [State Machines](/concepts/state-machines) for testable designs
- See [Event-Driven LLM](/concepts/event-driven-llm) for event testing patterns
