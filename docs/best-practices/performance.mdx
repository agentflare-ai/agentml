---
title: "Performance Optimization"
description: "Optimize your AgentML agents for speed and efficiency"
---

# Performance Optimization

Optimize your AgentML agents for maximum performance and minimal costs. AgentML's runtime snapshots and SCXML architecture enable significant performance gains.

## Token Efficiency with Runtime Snapshots

AgentML achieves 90%+ token reduction using runtime snapshots instead of full conversation history:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:openai="github.com/agentflare-ai/agentml-go/openai">

  <datamodel>
    <data id="conversationHistory" expr="[]" />
  </datamodel>

  <state id="chat">
    <transition event="user.message" target="generate">
      <!-- Only store essential context -->
      <assign location="userInput" expr="_event.data.message" />
    </transition>
  </state>

  <state id="generate">
    <onentry>
      <!-- Runtime sends compact state snapshot to LLM, not full history -->
      <openai:generate
        model="gpt-4o"
        location="_event"
        promptexpr="userInput" />
    </onentry>

    <transition event="action.response" target="chat">
      <assign location="response" expr="_event.data.message" />
    </transition>
  </state>
</agentml>
```

**Token comparison (10-turn conversation):**
- Traditional: ~100,000 tokens
- AgentML snapshots: ~2,700 tokens
- **Reduction: 97%**

## Provider Caching

Use prompt caching to reduce latency and costs:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:openai="github.com/agentflare-ai/agentml-go/openai">

  <datamodel>
    <!-- Cache-friendly system prompt -->
    <data id="systemContext" expr="'You are a helpful assistant specialized in technical documentation. Always provide concise, accurate answers.'" />
  </datamodel>

  <state id="generate">
    <onentry>
      <!-- System context is cached by provider -->
      <openai:generate
        model="gpt-4o"
        location="_event"
        systemexpr="systemContext"
        promptexpr="userInput" />
    </onentry>

    <transition event="action.response" target="respond" />
  </state>
</agentml>
```

**Caching benefits:**
- Google Gemini: 90% off cached tokens
- OpenAI: 50% off cached tokens
- Reduced latency on repeated prompts

## Concise Prompts

Minimize token usage with focused prompts:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:openai="github.com/agentflare-ai/agentml-go/openai">

  <!-- Bad: Verbose prompt -->
  <state id="verbose_generate">
    <onentry>
      <openai:generate
        model="gpt-4o"
        location="_event"
        promptexpr="'Please carefully analyze the following text and provide a comprehensive summary that includes all the main points, key details, and important information contained within the text: ' + longText" />
    </onentry>
  </state>

  <!-- Good: Concise prompt -->
  <state id="concise_generate">
    <onentry>
      <openai:generate
        model="gpt-4o"
        location="_event"
        promptexpr="'Summarize key points:\n' + longText" />
    </onentry>
  </state>
</agentml>
```

## Streaming Responses

Stream LLM responses for better perceived performance:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:openai="github.com/agentflare-ai/agentml-go/openai"
       xmlns:stdin="github.com/agentflare-ai/agentml-go/stdin">

  <state id="stream_generate">
    <onentry>
      <openai:stream
        model="gpt-4o"
        location="_event"
        promptexpr="userInput" />
    </onentry>

    <!-- Receive chunks as they arrive -->
    <transition event="action.stream.chunk" target="stream_generate">
      <stdin:write expr="_event.data.chunk" />
    </transition>

    <!-- Complete when done -->
    <transition event="action.stream.complete" target="complete">
      <assign location="fullResponse" expr="_event.data.message" />
    </transition>
  </state>
</agentml>
```

## Parallel State Processing

Process multiple tasks concurrently:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:openai="github.com/agentflare-ai/agentml-go/openai">

  <datamodel>
    <data id="summaryResult" expr="''" />
    <data id="keywordsResult" expr="''" />
    <data id="sentimentResult" expr="''" />
  </datamodel>

  <parallel id="analyze">
    <!-- Run all analyses concurrently -->
    <state id="summarize">
      <onentry>
        <openai:generate
          model="gpt-4o"
          location="_event"
          promptexpr="'Summarize:\n' + text" />
      </onentry>
      <transition event="action.response" target="summarize_done">
        <assign location="summaryResult" expr="_event.data.message" />
      </transition>
      <state id="summarize_done" />
    </state>

    <state id="extract_keywords">
      <onentry>
        <openai:generate
          model="gpt-4o"
          location="_event"
          promptexpr="'Extract keywords:\n' + text" />
      </onentry>
      <transition event="action.response" target="keywords_done">
        <assign location="keywordsResult" expr="_event.data.message" />
      </transition>
      <state id="keywords_done" />
    </state>

    <state id="analyze_sentiment">
      <onentry>
        <openai:generate
          model="gpt-4o"
          location="_event"
          promptexpr="'Sentiment:\n' + text" />
      </onentry>
      <transition event="action.response" target="sentiment_done">
        <assign location="sentimentResult" expr="_event.data.message" />
      </transition>
      <state id="sentiment_done" />
    </state>
  </parallel>

  <!-- Continue after all parallel states complete -->
  <state id="combine_results">
    <onentry>
      <log expr="'All analyses complete'" />
    </onentry>
  </state>
</agentml>
```

## Batch Processing with Foreach

Process collections efficiently:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:memory="github.com/agentflare-ai/agentml-go/memory">

  <datamodel>
    <data id="documents" expr="[
      'Document 1 content...',
      'Document 2 content...',
      'Document 3 content...'
    ]" />
    <data id="results" expr="[]" />
  </datamodel>

  <state id="process_batch">
    <onentry>
      <!-- Batch embed all documents -->
      <foreach array="documents" item="doc" index="i">
        <memory:embed location="embedding" expr="doc" />
        <assign location="results" expr="results.concat([{
          index: i,
          content: doc,
          embedding: embedding
        }])" />
      </foreach>
    </onentry>

    <transition target="complete">
      <log expr="'Processed ' + results.length + ' documents'" />
    </transition>
  </state>
</agentml>
```

## Local LLMs for High Volume

Use Ollama for high-volume, low-latency scenarios:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:ollama="github.com/agentflare-ai/agentml-go/ollama">

  <state id="classify">
    <onentry>
      <!-- Fast local classification -->
      <ollama:generate
        model="llama2"
        location="_event"
        promptexpr="'Classify intent: ' + userInput + '\nResponse: [greeting|question|command]'" />
    </onentry>

    <transition event="action.response" target="route">
      <assign location="intent" expr="_event.data.message.trim()" />
    </transition>
  </state>

  <state id="route">
    <transition cond="intent === 'greeting'" target="handle_greeting" />
    <transition cond="intent === 'question'" target="handle_question" />
    <transition cond="intent === 'command'" target="handle_command" />
  </state>
</agentml>
```

**Performance benefits:**
- No API latency
- No per-token costs
- Consistent response times
- Privacy (data stays local)

## Memory Database Optimization

Optimize vector search performance:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:memory="github.com/agentflare-ai/agentml-go/memory">

  <state id="search_optimized">
    <onentry>
      <!-- Generate query embedding -->
      <memory:embed location="queryEmbedding" expr="userQuery" />

      <!-- Limit results for performance -->
      <memory:search
        location="results"
        expr="queryEmbedding"
        limit="5" />
    </onentry>

    <transition target="process_results">
      <!-- Process only top 5 most relevant results -->
      <log expr="'Found ' + results.length + ' relevant documents'" />
    </transition>
  </state>
</agentml>
```

**Optimization tips:**
- Use appropriate `limit` values (5-10 for most cases)
- Index frequently queried fields
- Periodically vacuum database: `sqlite3 memory.db "VACUUM;"`
- Monitor database file size

## Minimize Datamodel Size

Keep datamodel lean for efficient snapshots:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript">

  <!-- Bad: Storing everything -->
  <datamodel>
    <data id="fullHistory" expr="[]" />
    <data id="allResponses" expr="[]" />
    <data id="debugInfo" expr="{}" />
  </datamodel>

  <!-- Good: Store only essentials -->
  <datamodel>
    <data id="currentIntent" expr="''" />
    <data id="lastResponse" expr="''" />
    <data id="sessionId" expr="Date.now()" />
  </datamodel>
</agentml>
```

**Benefits:**
- Smaller runtime snapshots
- Faster state transitions
- Reduced token usage
- Lower memory consumption

## Conditional Event Handling

Optimize transition evaluation order:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript">

  <state id="handle_input">
    <!-- Put most common cases first for faster matching -->
    <transition event="user.message" cond="intent === 'chat'" target="chat" />
    <transition event="user.message" cond="intent === 'search'" target="search" />
    <transition event="user.message" cond="intent === 'settings'" target="settings" />

    <!-- Catch-all last -->
    <transition event="user.message" target="unknown" />
  </state>
</agentml>
```

## HTTP Connection Reuse

IOProcessors reuse connections automatically:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript">

  <state id="fetch_data">
    <onentry>
      <!-- Multiple HTTP requests reuse connections -->
      <send target="http://api.example.com/users" type="http">
        <content expr="{method: 'GET'}" />
      </send>

      <send target="http://api.example.com/posts" type="http">
        <content expr="{method: 'GET'}" />
      </send>
    </onentry>
  </state>
</agentml>
```

## Deployment Optimization

Optimize production deployments:

```bash
# Use systemd resource limits
[Service]
MemoryMax=512M
CPUQuota=50%

# Enable BBolt for persistent memory
export AGENTML_MEMORY_DB=/var/lib/agentml/memory.db

# Use fast SSD storage
WorkingDirectory=/mnt/fast-ssd/agentml
```

## Monitoring Performance

Track agent performance:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:openai="github.com/agentflare-ai/agentml-go/openai">

  <datamodel>
    <data id="startTime" expr="0" />
    <data id="endTime" expr="0" />
    <data id="tokensUsed" expr="0" />
  </datamodel>

  <state id="process">
    <onentry>
      <assign location="startTime" expr="Date.now()" />

      <openai:generate
        model="gpt-4o"
        location="_event"
        promptexpr="userInput" />
    </onentry>

    <transition event="action.response" target="complete">
      <assign location="endTime" expr="Date.now()" />
      <assign location="tokensUsed" expr="_event.data.usage.totalTokens" />

      <log expr="JSON.stringify({
        duration_ms: endTime - startTime,
        tokens_used: tokensUsed,
        tokens_per_second: tokensUsed / ((endTime - startTime) / 1000)
      })" />
    </transition>
  </state>
</agentml>
```

## Load Testing

Test agent performance under load:

```bash
#!/bin/bash

# load-test.sh
concurrent_requests=10
total_requests=100

echo "Load testing: $concurrent_requests concurrent, $total_requests total"

start_time=$(date +%s)

for i in $(seq 1 $total_requests); do
  agentmlx run agent.aml --event '{"name": "user.message", "data": {"message": "Hello"}}' &

  # Limit concurrency
  if [ $((i % concurrent_requests)) -eq 0 ]; then
    wait
  fi
done

wait
end_time=$(date +%s)

duration=$((end_time - start_time))
rps=$((total_requests / duration))

echo "Completed $total_requests requests in ${duration}s"
echo "Requests per second: $rps"
```

## Best Practices

1. **Use runtime snapshots**: Let AgentML handle context compression automatically
2. **Enable provider caching**: Cache system prompts for repeated use
3. **Write concise prompts**: Minimize token usage with focused prompts
4. **Stream responses**: Improve perceived performance with streaming
5. **Parallel processing**: Use `<parallel>` for concurrent operations
6. **Limit search results**: Use appropriate `limit` values for vector search
7. **Optimize datamodel**: Store only essential data
8. **Use local LLMs**: Ollama for high-volume, low-latency scenarios
9. **Order transitions**: Put common cases first
10. **Monitor metrics**: Track tokens, latency, and throughput

## Next Steps

- Learn about [Error Handling](/best-practices/error-handling) strategies
- Read [Testing](/best-practices/testing) techniques
- Explore [Token Efficiency](/concepts/token-efficiency) deep dive
- See [Memory Extension](/extensions/memory) for optimized storage
