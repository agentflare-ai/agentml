---
title: "Token Efficiency"
description: "Minimize LLM token usage with AgentML's runtime snapshots and schema-guided generation"
---

# Token Efficiency

AgentML is designed for token efficiency from the ground up. Unlike traditional LLM applications that send entire conversation histories on every request, AgentML provides the LLM with a compact "snapshot" of the current state, making prompts minimal and costs predictable.

## The Token Problem

Traditional LLM applications waste massive amounts of tokens:

```javascript
// Traditional approach - sends everything every time
const messages = [
  { role: 'system', content: longSystemPrompt },       // 500+ tokens
  ...conversationHistory,                              // 5000+ tokens
  { role: 'user', content: 'What is the user asking?' }, // 10 tokens
  { role: 'assistant', content: 'Here is my analysis...' }, // 100 tokens
  { role: 'user', content: 'Thanks!' }                 // 2 tokens
];

// Every request repeats ALL of this!
const response = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: messages  // 5612 tokens sent EVERY TIME
});
```

**Problems:**
- Entire conversation history sent on every request
- System prompts repeated unnecessarily
- Context grows linearly with conversation length
- No way to utilize provider caching effectively
- Costs scale unpredictably

## AgentML's Solution: Runtime Snapshots

AgentML solves this with **runtime snapshots** - compact representations of the agent's current state that provide the LLM with exactly what it needs to make decisions.

### How Runtime Snapshots Work

When the agent needs LLM input, the runtime generates a snapshot containing:

1. **Current State**: Which state the agent is in
2. **Datamodel**: Current variable values
3. **Available Events**: What events can be raised
4. **Event Schemas**: What structure each event requires

The LLM receives this snapshot plus a minimal prompt. It can then generate an appropriate event.

### Example: Before and After

**Traditional Approach (5000+ tokens):**
```
System: You are a flight booking assistant. You can search for flights, book flights, cancel flights...

User: I want to fly to Paris
Assistant: I can help you book a flight to Paris. When would you like to depart?
User: Next Monday
Assistant: Which city will you depart from?
User: New York
```

**AgentML Runtime Snapshot (200 tokens):**
```xml
<snapshot>
  <state id="collect_flight_info" />
  <datamodel>
    <data id="destination" value="Paris" />
    <data id="departure_date" value="next Monday" />
    <data id="from_city" value="" />  <!-- Still needed -->
  </datamodel>
  <available_events>
    <event name="intent.flight" schema="#/components/schemas/FlightIntent" />
  </available_events>
</snapshot>
```

The LLM generates: `{"type": "intent.flight", "data": {"action": "search", "from": "New York", "to": "Paris", ...}}`

## Provider Caching

The static parts of AgentML agents (the agent definition itself) can be cached by LLM providers using prompt caching:

**Anthropic Claude with Prompt Caching:**
```xml
<agentml xmlns="github.com/agentflare-ai/agentml">
  <!-- This entire agent definition is cached -->
  <state id="awaiting_input">
    <transition event="user.message" .../>
  </state>
  ...
</agentml>

<!-- Only the dynamic snapshot changes per request -->
<snapshot>
  <state id="awaiting_input" />
  <datamodel>
    <data id="user_input" value="Hello" />
  </datamodel>
</snapshot>
```

Benefits:
- Agent definition cached indefinitely
- Only snapshot sent per request
- 90%+ token reduction on subsequent requests
- Predictable, low costs

## Schema-Guided Generation

The `event:schema` attribute minimizes token usage by providing precise output specifications:

```xml
<transition event="intent.flight"
            event:schema='{
              "type": "object",
              "description": "Flight booking intent",
              "properties": {
                "action": {
                  "type": "string",
                  "enum": ["search", "book", "cancel"],
                  "description": "The flight action"
                },
                "from": {"type": "string", "description": "Departure city"},
                "to": {"type": "string", "description": "Arrival city"}
              },
              "required": ["action", "from", "to"]
            }'
            target="handle_flight" />
```

Instead of:
- ❌ "Generate a JSON object with flight information including action, from city, to city, date..."
- ✅ Schema provides structure, LLM generates compliant data

**Result**: Shorter prompts, more reliable outputs, fewer tokens.

## Minimizing Prompts with Context

AgentML runtime snapshots provide context, so prompts can be minimal:

**Without Snapshots:**
```
Prompt: "You are a flight booking assistant. The user wants to fly from {from} to {to} on {date}. Generate a search query."
```

**With Snapshots:**
```xml
<!-- Runtime provides full context -->
<snapshot>
  <state id="search_flights" />
  <datamodel>
    <data id="from" value="NYC" />
    <data id="to" value="Paris" />
    <data id="date" value="2025-01-15" />
  </datamodel>
</snapshot>

<!-- Prompt is just: -->
<gemini:generate promptexpr="'Search for flights'" />
```

The LLM sees the snapshot and knows exactly what to do.

## Token Efficiency Patterns

### Pattern 1: Event-Driven State Transitions

Let the state machine handle flow control, not the LLM:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       xmlns:gemini="github.com/agentflare-ai/agentml-go/gemini">

  <state id="classify">
    <onentry>
      <!-- Minimal prompt: just classify -->
      <gemini:generate
        model="gemini-2.0-flash-exp"
        location="_event"
        promptexpr="'Classify user intent: ' + userInput" />
    </onentry>

    <!-- State machine handles routing -->
    <transition event="intent.greeting" target="handle_greeting" />
    <transition event="intent.flight" target="handle_flight" />
    <transition event="intent.unknown" target="handle_unknown" />
  </state>
</agentml>
```

### Pattern 2: Datamodel as Context

Store context in datamodel, not conversation history:

```xml
<datamodel>
  <data id="user_name" expr="''" />
  <data id="booking_status" expr="''" />
  <data id="flight_preferences" expr="{}" />
</datamodel>

<state id="process">
  <onentry>
    <!-- Runtime snapshot includes all datamodel variables -->
    <!-- No need to repeat context in prompt -->
    <gemini:generate
      model="gemini-2.0-flash-exp"
      location="_event"
      promptexpr="'Continue booking process'" />
  </onentry>
</state>
```

### Pattern 3: Schema-First Design

Define precise schemas, let LLM fill them:

```xml
<transition event="booking.complete"
            event:schema='{
              "type": "object",
              "properties": {
                "confirmation_number": {"type": "string"},
                "total_price": {"type": "number"},
                "passenger_count": {"type": "number"}
              },
              "required": ["confirmation_number", "total_price"]
            }'
            target="confirm" />
```

LLM generates exactly what's needed, nothing more.

## Measuring Token Usage

### Runtime Logging

Enable verbose logging to see token usage:

```bash
agentmlx run agent.aml --verbose --log-tokens
```

Output:
```
[INFO] LLM Request: 245 tokens (agent_def: 200, snapshot: 45)
[INFO] LLM Response: 78 tokens
[INFO] Total: 323 tokens
[INFO] Cached: 200 tokens (agent definition)
[INFO] Billable: 123 tokens
```

### Snapshot Inspection

Save snapshots to see what's sent to the LLM:

```bash
agentmlx run agent.aml --save-snapshots ./debug
```

Inspect `./debug/snapshot_*.xml` to see exact context provided.

### Token Budgets

Set per-request token limits:

```xml
<gemini:generate
  model="gemini-2.0-flash-exp"
  location="_event"
  promptexpr="userInput"
  max-output-tokens="500" />  <!-- Limit response length -->
```

## Model Selection for Efficiency

### Fast Local Models

Use Ollama for classification and simple tasks:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       xmlns:ollama="github.com/agentflare-ai/agentml-go/ollama">

  <state id="classify">
    <!-- Use fast local model -->
    <onentry>
      <ollama:generate
        model="llama2"
        location="_event"
        promptexpr="'Classify: ' + userInput" />
    </onentry>
  </state>
</agentml>
```

### Powerful Cloud Models

Use Gemini for complex reasoning:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       xmlns:gemini="github.com/agentflare-ai/agentml-go/gemini">

  <state id="generate_response">
    <!-- Use powerful model -->
    <onentry>
      <gemini:generate
        model="gemini-2.0-flash-exp"
        location="_event"
        promptexpr="'Generate detailed response'" />
    </onentry>
  </state>
</agentml>
```

### Hybrid Approach

Combine fast and powerful models:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       xmlns:ollama="github.com/agentflare-ai/agentml-go/ollama"
       xmlns:gemini="github.com/agentflare-ai/agentml-go/gemini">

  <state id="classify">
    <!-- Fast local classification -->
    <onentry>
      <ollama:generate model="llama2" location="_event" promptexpr="'Classify: ' + userInput" />
    </onentry>

    <transition event="intent.simple" target="quick_response" />
    <transition event="intent.complex" target="detailed_response" />
  </state>

  <state id="detailed_response">
    <!-- Powerful cloud model only when needed -->
    <onentry>
      <gemini:generate model="gemini-2.0-flash-exp" location="_event" promptexpr="'Detailed response'" />
    </onentry>
  </state>
</agentml>
```

## Best Practices

1. **Use Runtime Snapshots**: Let the runtime provide context
2. **Minimize Prompts**: Keep `promptexpr` short and focused
3. **Schema Everything**: Use `event:schema` for all LLM outputs
4. **Store in Datamodel**: Put context in datamodel, not prompts
5. **Leverage Caching**: Design agents to maximize provider caching
6. **Choose Right Models**: Fast local models for classification, powerful cloud models for reasoning
7. **Set Token Limits**: Use `max-output-tokens` to control costs
8. **Monitor Usage**: Use `--log-tokens` to track consumption
9. **Inspect Snapshots**: Save snapshots to debug token usage
10. **Test Efficiency**: Measure before/after token counts

## Comparison: Traditional vs AgentML

### Traditional Chatbot

**Request 1:** 2,500 tokens (full history)
**Request 2:** 3,200 tokens (growing history)
**Request 3:** 4,100 tokens (still growing)
**Request 10:** 12,000 tokens (unsustainable)

**Total:** 100,000+ tokens for 10-turn conversation

### AgentML Agent

**Request 1:** 500 tokens (agent definition + snapshot)
**Request 2:** 250 tokens (cached agent + snapshot)
**Request 3:** 250 tokens (cached agent + snapshot)
**Request 10:** 250 tokens (cached agent + snapshot)

**Total:** 2,750 tokens for 10-turn conversation

**Savings:** 97% token reduction

## Real-World Example

Flight booking agent with full conversation:

```xml
<agentml xmlns="github.com/agentflare-ai/agentml"
       datamodel="ecmascript"
       xmlns:gemini="github.com/agentflare-ai/agentml-go/gemini">

  <datamodel>
    <data id="from" expr="''" />
    <data id="to" expr="''" />
    <data id="date" expr="''" />
  </datamodel>

  <state id="collect_info">
    <transition event="user.message"
                event:schema='{"type": "object", "properties": {"from": {"type": "string"}, "to": {"type": "string"}, "date": {"type": "string"}}}'
                target="search">
      <assign location="from" expr="_event.data.from || from" />
      <assign location="to" expr="_event.data.to || to" />
      <assign location="date" expr="_event.data.date || date" />
    </transition>
  </state>

  <state id="search">
    <onentry>
      <!-- Minimal prompt - snapshot provides context -->
      <gemini:generate
        model="gemini-2.0-flash-exp"
        location="_event"
        promptexpr="'Search for flights'" />
    </onentry>
  </state>
</agentml>
```

**Token usage per turn:** ~250 tokens (after first request)
**Traditional approach:** ~3000 tokens per turn

## Next Steps

- Learn about [Event-Driven LLM](/concepts/event-driven-llm) architecture
- Explore [Events & Schemas](/concepts/events-schemas) for schema-guided generation
- Read [Best Practices](/best-practices/performance) for production optimization
- See [Interpreter](/architecture/interpreter) for snapshot generation details
